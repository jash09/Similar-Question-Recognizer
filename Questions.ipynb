{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Questions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuNgk63U_hBA"
      },
      "source": [
        "**Downloading the Dependencies**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4UdZnqc3PNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92aa7be5-a714-4384-b646-50e1ffdbeab9"
      },
      "source": [
        "\n",
        "!pip install -q -U trax                         \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "from trax.fastmath import numpy as fastnp\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "\n",
        "random.seed(111)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 629 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 42.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 153 kB 40.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 39.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 286 kB 25.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 366 kB 25.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 38.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 7.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 64.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 65.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBBqIIViCPbp"
      },
      "source": [
        "**Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO6DqllGNZa0",
        "outputId": "47ebeef4-d578-4d9c-af71-e86b6ca7c710"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1QazSnI__Ko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "be4c1181-eebb-4344-c515-3d67433bbbc3"
      },
      "source": [
        "data = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/Questions/Questions.csv\")\n",
        "print(f\"No. of Question duplicate pairs: {len(data)}\")\n",
        "data.head(5)                                                "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of Question duplicate pairs: 404351\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  qid1  ...                                          question2 is_duplicate\n",
              "0   0     1  ...  What is the step by step guide to invest in sh...            0\n",
              "1   1     3  ...  What would happen if the Indian government sto...            0\n",
              "2   2     5  ...  How can Internet speed be increased by hacking...            0\n",
              "3   3     7  ...  Find the remainder when [math]23^{24}[/math] i...            0\n",
              "4   4     9  ...            Which fish would survive in salt water?            0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMKciVITGpvF"
      },
      "source": [
        "**Processing the Data**\n",
        "* I'll divide the data into two groups: training and testing. The Test Set will be used to assess the Model afterwards. To train the Model, I'll only use Question Pairs that are duplicates. I'll make two batches of Siamese Networks to feed into the Neural Networks. The original pairs of Questions are used in the Test set, as well as the Status indicating whether or not the Questions are duplicates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIliyuetFyzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab34b33-91c4-4eca-c4a2-a547e4cb5f39"
      },
      "source": [
        "\n",
        "N_train = 300000                                               \n",
        "N_test = 10240                                                 \n",
        "data_train = data[:N_train]                                                   \n",
        "data_test = data[N_train:N_train+N_test]                                       \n",
        "del(data)                                                                      \n",
        "\n",
        "print(f\"Training Set: {len(data_train)} and Test Set: {len(data_test)}\")\n",
        "\n",
        "train_idx = (data_train[\"is_duplicate\"] == 1).to_numpy()\n",
        "train_idx = [i for i,x in enumerate(train_idx) if x]\n",
        "print(f\"Number of Duplicate Questions: {len(train_idx)}\")\n",
        "print(f\"Indexes of first Duplicate Questions: {train_idx[:10]}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: 300000 and Test Set: 10240\n",
            "Number of Duplicate Questions: 111486\n",
            "Indexes of first Duplicate Questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdCp1rDSOuod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc21ebb0-9320-410a-f190-69913831ebd6"
      },
      "source": [
        "print(data_train[\"question1\"][20])                                 \n",
        "print(data_train[\"question2\"][20])                                 \n",
        "print(\"Index 20 is duplicate:\", data_train[\"is_duplicate\"][20])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do rockets look white?\n",
            "Why are rockets and boosters painted white?\n",
            "Index 20 is duplicate: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpA5u5orQRDN"
      },
      "source": [
        "**Preparing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JREoTE1JMQnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d04795d-87e1-45f5-bd42-3f24b711fd27"
      },
      "source": [
        "\n",
        "Q1_train_words = np.array(data_train[\"question1\"][train_idx])\n",
        "Q2_train_words = np.array(data_train[\"question2\"][train_idx])\n",
        "\n",
        "Q1_test_words = np.array(data_test[\"question1\"])\n",
        "Q2_test_words = np.array(data_test[\"question2\"])\n",
        "y_test = np.array(data_test[\"is_duplicate\"])\n",
        "\n",
        "print(\"TRAINING QUESTIONS:\\n\")\n",
        "print(\"Question 1:\", Q1_train_words[7])\n",
        "print(\"Question 2:\", Q2_train_words[7], \"\\n\")\n",
        "\n",
        "print(\"TESTING QUESTIONS:\\n\")\n",
        "print(\"Question 1:\", Q1_test_words[7])\n",
        "print(\"Question 2:\", Q2_test_words[7], \"\\n\")\n",
        "print(\"Inspecting Testing pairs is duplicate:\", y_test[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING QUESTIONS:\n",
            "\n",
            "Question 1: Why are so many Quora users posting questions that are readily answered on Google?\n",
            "Question 2: Why do people ask Quora questions which can be answered easily by Google? \n",
            "\n",
            "TESTING QUESTIONS:\n",
            "\n",
            "Question 1: Which is the best digital photo frame?\n",
            "Question 2: What are the best 12-inch digital photo frames? \n",
            "\n",
            "Inspecting Testing pairs is duplicate: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-I0nWKTUqSI"
      },
      "source": [
        "**Preparing the Data**\n",
        "* I'll use an Index, which will be a list of integers, to encode each word of the chosen pairings. To begin, I'll use NLTK to tokenize each word, then use Python's Default Dictionary to assign the value 0 to all Out of Vocabulary Words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMQ-9PtSQcIo",
        "outputId": "bc77cded-f1e5-44ca-d8f0-76500168b129"
      },
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GE6ELe7UBY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf21048-d93a-4a22-df39-b58015d44c30"
      },
      "source": [
        "\n",
        "Q1_train = np.empty_like(Q1_train_words)                                \n",
        "Q2_train = np.empty_like(Q2_train_words)                                \n",
        "Q1_test = np.empty_like(Q1_test_words)                                  \n",
        "Q2_test = np.empty_like(Q2_test_words)                                  \n",
        "\n",
        "\n",
        "vocab = defaultdict(lambda: 0)\n",
        "vocab[\"<PAD>\"] = 1\n",
        "for idx in range(len(Q1_train_words)):\n",
        "  Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])               \n",
        "  Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])              \n",
        "  q = Q1_train[idx] + Q2_train[idx]\n",
        "  for word in q:\n",
        "    if word not in vocab:\n",
        "      vocab[word] = len(vocab) + 1\n",
        "print(\"The length of the Vocabulary is:\", len(vocab))\n",
        "\n",
        "\n",
        "for idx in range(len(Q1_test_words)):\n",
        "  Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])                 \n",
        "  Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])                 \n",
        "\n",
        "print(\"Training Set is reduced to:\", len(Q1_train))\n",
        "print(\"Test Set is:\", len(Q1_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the Vocabulary is: 36342\n",
            "Training Set is reduced to: 111486\n",
            "Test Set is: 10240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywba7dxVQu_l"
      },
      "source": [
        "**Preparing the Data**\n",
        "* I will convert each Questions Pairs to Tensors or array of Numbers using the Vocabulary. Then I will split the Training set into Training and Validation Dataset so that I can use it to train and evaluate the Neural Networks: Siamese Networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVXqncSH1ntc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d70d301-7a42-4b47-be61-aac1eb71bbea"
      },
      "source": [
        "\n",
        "for i in range(len(Q1_train)):\n",
        "  Q1_train[i] = [vocab[word] for word in Q1_train[i]]\n",
        "  Q2_train[i] = [vocab[word] for word in Q2_train[i]]\n",
        "\n",
        "\n",
        "for i in range(len(Q1_test)):\n",
        "  Q1_test[i] = [vocab[word] for word in Q1_test[i]]\n",
        "  Q2_test[i] = [vocab[word] for word in Q2_test[i]]\n",
        "\n",
        "print(\"Question in the Training Set:\")                         \n",
        "print(Q1_train_words[7], \"\\n\")\n",
        "print(\"Encoded Version:\")\n",
        "print(Q1_train[7], \"\\n\")\n",
        "print(\"Question in the Test Set:\")                               \n",
        "print(Q1_test_words[7], \"\\n\")\n",
        "print(\"Encoded Version:\")\n",
        "print(Q1_test[7], \"\\n\")\n",
        "\n",
        "split = int(len(Q1_train) * 0.8)\n",
        "train_Q1, train_Q2 = Q1_train[:split], Q2_train[:split]                        \n",
        "val_Q1, val_Q2 = Q1_train[split:], Q2_train[split:]                            \n",
        "print(f\"Total numbers of questions pairs: {len(Q1_train)}\")              \n",
        "print(f\"The length of Training set: {len(train_Q1)}\")                          \n",
        "print(f\"The length of Validation set: {len(val_Q1)}\")                          "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question in the Training Set:\n",
            "Why are so many Quora users posting questions that are readily answered on Google? \n",
            "\n",
            "Encoded Version:\n",
            "[86, 87, 88, 89, 90, 91, 92, 93, 17, 87, 94, 95, 72, 96, 21] \n",
            "\n",
            "Question in the Test Set:\n",
            "Which is the best digital photo frame? \n",
            "\n",
            "Encoded Version:\n",
            "[283, 156, 78, 216, 1442, 1223, 4114, 21] \n",
            "\n",
            "Total numbers of questions pairs: 111486\n",
            "The length of Training set: 89188\n",
            "The length of Validation set: 22298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxVe0OX0XNeE"
      },
      "source": [
        "**Data Generator**\n",
        "* Using batches for training the dataset is more efficient in most Natural Language Processing and AI applications. Now I'll create the Data Generator, which will take in Question pairs and return batches of Tuples. The Tuples are made up of two arrays, each with batch size Questions pairs. The next batch will be returned by the command next(data generator). The Data Generator will deliver the data in a format that can be easily entered into the Model for Feed Forward computation. It will return a pair of Question arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBN7QM5TWv49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef2e9e2-6fd4-410f-da08-e1e317508189"
      },
      "source": [
        "\n",
        "def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n",
        "  input1, input2 = [], []\n",
        "  idx = 0\n",
        "  len_q = len(Q1)\n",
        "  question_index = [*range(len_q)]\n",
        "  if shuffle:\n",
        "    random.shuffle(question_index)\n",
        "  \n",
        "  while True:\n",
        "    if idx >= len_q:\n",
        "      idx = 0\n",
        "      if shuffle:\n",
        "        random.shuffle(question_index)\n",
        "    \n",
        "    q1 = Q1[question_index[idx]]\n",
        "    q2 = Q2[question_index[idx]]\n",
        "    idx += 1\n",
        "    #@ Adding the Data:\n",
        "    input1.append(q1)\n",
        "    input2.append(q2)\n",
        "    if len(input1) == batch_size:\n",
        "      max_len = max(max([len(q) for q in input1]),\n",
        "                    max([len(q) for q in input2]))\n",
        "      max_len = 2**int(np.ceil(np.log2(max_len)))\n",
        "      b1, b2 = [], []\n",
        "      for q1, q2 in zip(input1, input2):\n",
        "        q1 = q1 + [pad] * (max_len - len(q1))                        \n",
        "        q2 = q2 + [pad] * (max_len - len(q2))                         \n",
        "        b1.append(q1)\n",
        "        b2.append(q2)\n",
        "      yield np.array(b1), np.array(b2)\n",
        "      input1, input2 = [], []                                       \n",
        "\n",
        "\n",
        "res1, res2 = next(data_generator(train_Q1, train_Q2, batch_size=2))\n",
        "print(f\"First Questions:\\n{res1}\")\n",
        "print(f\"\\nSecond Questions:\\n{res2}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Questions:\n",
            "[[   30    87    78  1725    17  2274   286 11452 11453    21     1     1\n",
            "      1     1     1     1]\n",
            " [  929     4 11746   129 16520    21     1     1     1     1     1     1\n",
            "      1     1     1     1]]\n",
            "\n",
            "Second Questions:\n",
            "[[   32    89  1725   819   286 11452 11453    21     1     1     1     1\n",
            "      1     1     1     1]\n",
            " [  929     4 11746   127 16520    21     1     1     1     1     1     1\n",
            "      1     1     1     1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRCml-SKh3gY"
      },
      "source": [
        "**Siamese Neural Network**\n",
        "* A Siamese Neural Network is a Neural Network which uses the same weight while working in tandem on two different Input vectors to compute comparable output Vectors. Here, I will get the Embedding, run it through LSTM or Long Short Term Memory Network, Noramlize the two Vectors and Finally, I will use Triplet Loss to get the corresponding Cosine Similarity for each pair of Questions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MB-pbo2e7b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58914444-5557-4760-f767-8dd4d28b91d3"
      },
      "source": [
        "\n",
        "def Siamese(vocab_size=len(vocab), d_model=128, mode=\"train\"):\n",
        "  \n",
        "  def normalize(x):\n",
        "    return x / fastnp.sqrt(fastnp.sum(x*x, axis=-1, keepdims=True))\n",
        "  \n",
        "  processor = tl.Serial(                                                 \n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model),            \n",
        "      tl.LSTM(n_units=d_model),                                           \n",
        "      tl.Mean(axis=1),                                                   \n",
        "                                      \n",
        "      tl.Fn(\"Normalize\", lambda x: normalize(x))                        \n",
        "  )\n",
        "  \n",
        "  model = tl.Parallel(processor, processor)\n",
        "  return model\n",
        "\n",
        "model = Siamese()\n",
        "print(model)                                                             "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parallel_in2_out2[\n",
            "  Serial[\n",
            "    Embedding_41789_128\n",
            "    LSTM_128\n",
            "    Mean\n",
            "    Normalize\n",
            "  ]\n",
            "  Serial[\n",
            "    Embedding_41789_128\n",
            "    LSTM_128\n",
            "    Mean\n",
            "    Normalize\n",
            "  ]\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DmBELaQ0hGA"
      },
      "source": [
        "**Triplet Loss**\n",
        "* The Triplet Loss makes use of a Baseline or Anchor Input which is compared to the Positive or Truthy Input and a Negatve or Falsy Input. The distance from the Anchor Input to the Positive Input is minimized and the distance from the Anchor Input to the Negative Input is maximized. The Triplet Loss is composed of two terms where one term utilizes the mean of all the non duplicates and the second term utilizes the Closest Negative. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5or7rjVz4W_"
      },
      "source": [
        "\n",
        "def TripletLossFn(v1, v2, margin=0.25):\n",
        "  \"\"\" Custom Loss Function. \"\"\"\n",
        "  scores = fastnp.dot(v1, v2.T)                                                       \n",
        "  batch_size = len(scores)                                                           \n",
        "  positive = fastnp.diagonal(scores)                                                 \n",
        "  negative_without_positive = scores - 2.0 * fastnp.eye(batch_size)\n",
        "  closest_negative = negative_without_positive.max(axis=1)                            \n",
        "  mean_negative = fastnp.sum(negative_zero_on_duplicate, axis=1)/(batch_size - 1)\n",
        "  triplet_loss1 = fastnp.maximum(0, margin - positive + closest_negative)\n",
        "  triplet_loss2 = fastnp.maximum(0, margin - positive + mean_negative)\n",
        "  triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n",
        "  return triplet_loss\n",
        "\n",
        "\n",
        "def TripletLoss(margin=0.25):\n",
        "  triplet_loss_fn = partial(TripletLossFn, margin=margin)\n",
        "  return tl.Fn(\"TripletLoss\", triplet_loss_fn)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THnc2u46Frcg"
      },
      "source": [
        "**Training the Model**\n",
        "* Now, I will train the Model. I will define the Cost Function and the Optimizer as ususal. I will use Training Iterator to go through all the Data for each Epochs while training the Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJOlFJpl9SnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "886d6c63-3249-4c3d-8d52-b0326d94e82a"
      },
      "source": [
        "#@ Preparing the Data:\n",
        "batch_size = 256\n",
        "train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab[\"<PAD>\"])\n",
        "val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab[\"<PAD>\"])\n",
        "\n",
        "#@ Training the Model:\n",
        "lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n",
        "def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator,\n",
        "                val_generator=val_generator, output_dir=\"model/\"):\n",
        "\n",
        "  output_dir = os.path.expanduser(output_dir)\n",
        "  \n",
        "  #@ Training:\n",
        "  train_task = training.TrainTask(\n",
        "      labeled_data = train_generator,                                               \n",
        "      loss_layer = TripletLoss(),                                                       \n",
        "      optimizer = trax.optimizers.Adam(0.001),                                          \n",
        "      lr_schedule = lr_schedule                                                        \n",
        "  )\n",
        "  #@ Evaluating:\n",
        "  eval_task = training.EvalTask(\n",
        "      labeled_data = val_generator,                                                     \n",
        "      metrics = [TripletLoss()],                                                        \n",
        "      n_eval_batches = 3\n",
        "  )\n",
        "  #@ Training the Model:\n",
        "  training_loop = training.Loop(                                                       \n",
        "      Siamese(),                                                                        \n",
        "      train_task, eval_tasks = eval_task,\n",
        "      output_dir = output_dir\n",
        "  )\n",
        "  return training_loop\n",
        "\n",
        "#@ Training the Model:\n",
        "training_loop = train_model(Siamese, TripletLoss, lr_schedule)\n",
        "training_loop.run(1000)                                                                "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:372: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_id has been renamed to jax.process_index. This alias \"\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:385: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step   1100: Ran 100 train steps in 88.72 secs\n",
            "Step   1100: train TripletLoss |  0.07060241\n",
            "Step   1100: eval  TripletLoss |  0.08669567\n",
            "\n",
            "Step   1200: Ran 100 train steps in 81.57 secs\n",
            "Step   1200: train TripletLoss |  0.06752674\n",
            "Step   1200: eval  TripletLoss |  0.09445456\n",
            "\n",
            "Step   1300: Ran 100 train steps in 73.97 secs\n",
            "Step   1300: train TripletLoss |  0.06285258\n",
            "Step   1300: eval  TripletLoss |  0.08742717\n",
            "\n",
            "Step   1400: Ran 100 train steps in 77.71 secs\n",
            "Step   1400: train TripletLoss |  0.05550867\n",
            "Step   1400: eval  TripletLoss |  0.07100180\n",
            "\n",
            "Step   1500: Ran 100 train steps in 75.23 secs\n",
            "Step   1500: train TripletLoss |  0.05138228\n",
            "Step   1500: eval  TripletLoss |  0.08083352\n",
            "\n",
            "Step   1600: Ran 100 train steps in 75.73 secs\n",
            "Step   1600: train TripletLoss |  0.05150433\n",
            "Step   1600: eval  TripletLoss |  0.06651612\n",
            "\n",
            "Step   1700: Ran 100 train steps in 72.20 secs\n",
            "Step   1700: train TripletLoss |  0.04927659\n",
            "Step   1700: eval  TripletLoss |  0.06092033\n",
            "\n",
            "Step   1800: Ran 100 train steps in 72.69 secs\n",
            "Step   1800: train TripletLoss |  0.04358396\n",
            "Step   1800: eval  TripletLoss |  0.06702789\n",
            "\n",
            "Step   1900: Ran 100 train steps in 76.42 secs\n",
            "Step   1900: train TripletLoss |  0.04227415\n",
            "Step   1900: eval  TripletLoss |  0.06915512\n",
            "\n",
            "Step   2000: Ran 100 train steps in 73.47 secs\n",
            "Step   2000: train TripletLoss |  0.04307369\n",
            "Step   2000: eval  TripletLoss |  0.06291787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHAyULCAhTcb"
      },
      "source": [
        "**Model Evaluation**\n",
        "* I will utilize the Test Set which was configured earlier to determine the accuracy of the Model. Actually the Training Set only had Positive examples whereas the Test Set and y test is setup as pairs of Questions and some of which are duplicates and some are not. I will compute the Cosine Similarity of each pair, threshold it and compare the result to y test. The results are accumulated to produce the Accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRR-LHzhfmpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0711e2-484c-4c74-884b-4f420518f92d"
      },
      "source": [
        "#@ Loading the Saved Model:\n",
        "model = Siamese()\n",
        "model.init_from_file(\"/content/model/model.pkl.gz\")\n",
        "\n",
        " \n",
        "def classify(test_Q1, test_Q2, y, threshold, model, vocab, data_generator=data_generator, batch_size=64):\n",
        "  \"\"\" Function to test the Accuracy of the Model. \"\"\"\n",
        "  accuracy = 0                                                                               \n",
        "  for i in range(0, len(test_Q1), batch_size):\n",
        "    q1, q2 = next(data_generator(test_Q1[i:i+batch_size], test_Q2[i:i+batch_size],\n",
        "                                 batch_size, vocab[\"<PAD>\"], shuffle=False))\n",
        "    y_test = y[i:i+batch_size]                                                              \n",
        "    v1, v2 = model((q1, q2))                                                                 \n",
        "    for j in range(batch_size):\n",
        "      d = np.dot(v1[j], v2[j].T)                                                            \n",
        "      res = d > threshold\n",
        "      accuracy += (y_test[j] == res)\n",
        "  accuracy = accuracy / len(test_Q1)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "accuracy = classify(Q1_test, Q2_test, y_test, 0.7, model, vocab, batch_size=512)             \n",
        "print(\"Accuracy :\", accuracy) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.7462890625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7_cGJx-nV7f"
      },
      "source": [
        "**Model Evaluation**\n",
        "* Now, I will test the Model using my own Questions. I will build a reverse Vocabulary that allows the map encoded Questions back to words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQmOHJaSmSdt"
      },
      "source": [
        "\n",
        "def predict(question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=False):\n",
        "  \"\"\" Function for predicting if two Questions are Duplicates. \"\"\"\n",
        "  q1 = nltk.word_tokenize(question1)                                \n",
        "  q2 = nltk.word_tokenize(question2)                                \n",
        "  Q1, Q2 = [], []\n",
        "  for word in q1:\n",
        "    Q1 += [vocab[word]]                                            \n",
        "  for word in q2:\n",
        "    Q2 += [vocab[word]]                                            \n",
        "  Q1, Q2 = next(data_generator([Q1], [Q2], 1, vocab[\"<PAD>\"]))\n",
        "  v1, v2 = model((Q1, Q2))                                          \n",
        "  d = fastnp.dot(v1[0], v2[0].T)\n",
        "  res = d > threshold\n",
        "  if (verbose):\n",
        "    print(\"Q1 = \", Q1, \"\\nQ2 = \", Q2)\n",
        "    print(\"d = \", d)\n",
        "    print(\"res = \", res)\n",
        "  return res "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3yO1U1mrfON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f7046bd-19d0-43a7-da85-3a774f4b340e"
      },
      "source": [
        "#@ Examples of Questions:\n",
        "question1 = \"What is your name?\"\n",
        "question2 = \"What are you known as?\"\n",
        "#@ Predicting the Duplicated Questions:\n",
        "example1 = predict(question1, question2, 0.7, model, vocab, verbose=True)\n",
        "print(\"Example1:\", example1, \"\\n\")\n",
        "\n",
        "#@ Example of Questions:\n",
        "question1 = \"Where are you taking us?\"\n",
        "question2 = \"Where are we going?\"\n",
        "#@ Predicting the Duplicated Questions:\n",
        "example2 = predict(question1, question2, 0.7, model, vocab, verbose=True)\n",
        "print(\"Example2:\", example2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1 =  [[  30  156   56 1377   21    1    1    1]] \n",
            "Q2 =  [[  30   87   53 2715  251   21    1    1]]\n",
            "d =  0.75979096\n",
            "res =  True\n",
            "Example1: True \n",
            "\n",
            "Q1 =  [[676  87  53 906 603  21   1   1]] \n",
            "Q2 =  [[ 676   87  138 1479   21    1    1    1]]\n",
            "d =  0.8487048\n",
            "res =  True\n",
            "Example2: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LwuPhbQ8Jok"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}